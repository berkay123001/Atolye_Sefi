# agents/graph_agent.py

import sys
import os
import operator
from typing import TypedDict, Annotated, List, Dict, Any

# Projenin ana dizinini Python'un yoluna ekliyoruz.
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

# --- LangChain ve LangGraph KÃ¼tÃ¼phaneleri ---
from langgraph.graph import StateGraph, END
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate

# --- Proje BileÅŸenleri ---
from config import settings
from tools.architectural_tools import decide_architecture
from tools.operational_tools import find_and_prepare_gpu
from tools.pod_management_tools import execute_command_on_pod, get_pod_status


# 1. Yeni "Beyaz Tahta" (AgentState) - Ã‡ok AdÄ±mlÄ± HafÄ±za + AkÄ±llÄ± YÃ¶nlendirme
class AgentState(TypedDict):
    input: str                          # KullanÄ±cÄ±nÄ±n orijinal gÃ¶revi
    route_decision: str                 # YÃ¶nlendirme kararÄ±: "chat" veya "task"
    plan: List[str]                     # AdÄ±mlarÄ±n planÄ± (string listesi)
    executed_steps: Annotated[List[Dict], operator.add]  # Tamamlanan adÄ±mlarÄ±n sonuÃ§larÄ±
    current_step_index: int             # Åu anki adÄ±m numarasÄ±
    final_result: str                   # Nihai cevap


# 2. Ã‡ok AdÄ±mlÄ± Proje YÃ¶neticisi SÄ±nÄ±fÄ±
class GraphAgent:
    """
    LangGraph kullanarak, Ã§ok adÄ±mlÄ± gÃ¶revleri planlayan, adÄ±m adÄ±m uygulayan
    ve hafÄ±zasÄ±nÄ± koruyan geliÅŸmiÅŸ proje yÃ¶neticisi ajanÄ±.
    """
    def __init__(self):
        # LLM'i baÅŸlat
        self.llm = ChatGroq(
            temperature=0.1,  # Biraz yaratÄ±cÄ±lÄ±k iÃ§in artÄ±rdÄ±k
            model_name=settings.AGENT_MODEL_NAME,
            groq_api_key=settings.GROQ_API_KEY
        )
        
        # Alet Ã§antasÄ±nÄ± sÃ¶zlÃ¼k olarak tanÄ±mla (kolay eriÅŸim iÃ§in)
        self.tools_dict = {
            "decide_architecture": decide_architecture,
            "find_and_prepare_gpu": find_and_prepare_gpu,
            "execute_command_on_pod": execute_command_on_pod,
            "get_pod_status": get_pod_status,
            # SimÃ¼lasyon aracÄ± (gerÃ§ek implementasyon iÃ§in hazÄ±r)
            "start_task_on_pod": self._simulate_task_execution
        }
        
        # GrafiÄŸi oluÅŸtur
        self.graph = self.build_graph()
        print("ğŸ§  GraphAgent: Ã‡ok adÄ±mlÄ± hafÄ±za sistemi aktif!")

    def _simulate_task_execution(self, **kwargs) -> Dict:
        """
        GeÃ§ici simÃ¼lasyon aracÄ± - gerÃ§ek implementasyon gelene kadar
        """
        return {
            "status": "success",
            "message": "Task simulation completed successfully",
            "details": f"Simulated execution with parameters: {kwargs}"
        }

    # === YENÄ° Ä°Å Ä°STASYONU 0: AKILLI YÃ–NLENDÄ°RÄ°CÄ° DÃœÄÃœMÃœ ===
    def route_query(self, state: AgentState) -> Dict:
        """
        KullanÄ±cÄ±nÄ±n girdisini analiz eder ve "chat" mi "task" mÄ± olduÄŸuna karar verir.
        Bu, grafiÄŸin "kapÄ±daki gÃ¼venlik gÃ¶revlisi"sidir.
        """
        print("\nğŸšª [YÃ–NLENDÄ°RÄ°CÄ°] KullanÄ±cÄ± girdisi analiz ediliyor...")
        
        routing_prompt = ChatPromptTemplate.from_messages([
            ("system", """Sen, kullanÄ±cÄ± girdilerini kategorize eden uzman bir analiz sistemisin.
            
GÃ¶revin: Verilen girdiyi analiz edip, sadece "chat" veya "task" kelimelerinden birini dÃ¶ndÃ¼rmek.

KURALLAR:
- EÄŸer girdi selamlama, sohbet, basit soru ise -> "chat" 
- EÄŸer girdi eylem, komut, plan gerektiriyorsa -> "task"

Ã–RNEKLERÄ°:
- "merhaba" -> chat
- "nasÄ±lsÄ±n" -> chat  
- "GPU bul" -> task
- "model eÄŸit" -> task
- "16GB VRAM ortam hazÄ±rla" -> task

SADECE "chat" veya "task" kelimesini dÃ¶ndÃ¼r, baÅŸka hiÃ§bir ÅŸey yazma!"""),
            ("user", "{input}")
        ])
        
        try:
            response = self.llm.invoke(routing_prompt.format_messages(input=state["input"]))
            decision = response.content.strip().lower()
            
            # GÃ¼venlik kontrolÃ¼ - sadece geÃ§erli deÄŸerler
            if decision not in ["chat", "task"]:
                decision = "chat"  # ÅÃ¼pheli durumlarda gÃ¼venli tarafta kal
                
            print(f"ğŸ“‹ YÃ¶nlendirme KararÄ±: '{decision}' (Girdi: '{state['input']}')")
            
            return {"route_decision": decision}
            
        except Exception as e:
            print(f"âŒ YÃ¶nlendirici hatasÄ±: {e}")
            return {"route_decision": "chat"}  # Hata durumunda gÃ¼venli mod

    # === YENÄ° Ä°Å Ä°STASYONU 1: SOHBET DÃœÄÃœMÃœ ===
    def chatbot_step(self, state: AgentState) -> Dict:
        """
        Basit sohbet iÅŸlemlerini halleder. HiÃ§bir araÃ§ kullanmaz, sadece doÄŸal sohbet.
        """
        print("\nğŸ’¬ [SOHBET DÃœÄÃœMÃœ] DoÄŸal sohbet cevabÄ± oluÅŸturuluyor...")
        
        chat_prompt = ChatPromptTemplate.from_messages([
            ("system", """Sen, AtÃ¶lye Åefi isimli, yardÄ±msever ve dostane bir AI asistanÄ±sÄ±n.
            
Ã–zelliklerin:
- MLOps ve AI konularÄ±nda uzman
- Docker, GPU, model eÄŸitimi konularÄ±nda bilgili
- SÄ±cak ve samimi bir konuÅŸma tarzÄ±n var
- TÃ¼rkÃ§e konuÅŸuyorsun

KullanÄ±cÄ±yla doÄŸal bir sohbet yap. KÄ±sa, net ve dostane cevaplar ver."""),
            ("user", "{input}")
        ])
        
        try:
            response = self.llm.invoke(chat_prompt.format_messages(input=state["input"]))
            result = response.content.strip()
            
            print(f"ğŸ’­ Sohbet CevabÄ±: {result[:100]}...")
            
            return {"final_result": result}
            
        except Exception as e:
            print(f"âŒ Sohbet hatasÄ±: {e}")
            return {"final_result": "ÃœzgÃ¼nÃ¼m, ÅŸu anda bir sorun yaÅŸÄ±yorum. Tekrar dener misin?"}

    # === Ä°Å Ä°STASYONU 2: PLANLAMA DÃœÄÃœMÃœ ===
    def plan_step(self, state: AgentState) -> Dict:
        """
        KullanÄ±cÄ±nÄ±n gÃ¶revini analiz eder ve adÄ±m adÄ±m plan oluÅŸturur.
        """
        print("\nğŸ¯ [PLANLAMA DÃœÄÃœMÃœ] GÃ¶rev analiz ediliyor ve plan oluÅŸturuluyor...")
        
        planning_prompt = ChatPromptTemplate.from_messages([
            ("system", """Sen bir MLOps proje yÃ¶neticisisin. KullanÄ±cÄ±nÄ±n gÃ¶revini analiz et ve 
            adÄ±m adÄ±m bir plan oluÅŸtur. Her adÄ±m, hangi aracÄ±n Ã§aÄŸrÄ±lacaÄŸÄ±nÄ± net olarak belirtmeli.

            KullanÄ±labilir araÃ§lar:
            - decide_architecture: Mimari kararlarÄ± almak iÃ§in
            - find_and_prepare_gpu: GPU ortamÄ± bulmak ve hazÄ±rlamak iÃ§in  
            - execute_command_on_pod: Pod'da komut Ã§alÄ±ÅŸtÄ±rmak iÃ§in
            - get_pod_status: Pod durumunu kontrol etmek iÃ§in
            - start_task_on_pod: Pod'da Ã¶zel gÃ¶rev baÅŸlatmak iÃ§in

            PlanÄ±, her satÄ±rda bir adÄ±m olacak ÅŸekilde, ÅŸu formatta yaz:
            1. [ARAÃ‡_ADI] aÃ§Ä±klama
            2. [ARAÃ‡_ADI] aÃ§Ä±klama
            ...

            Ã–rnek:
            1. [find_and_prepare_gpu] 16GB VRAM'li GPU ortamÄ± bul ve hazÄ±rla
            2. [execute_command_on_pod] Git repository'sini clone et
            3. [execute_command_on_pod] Gerekli kÃ¼tÃ¼phaneleri yÃ¼kle"""),
            ("user", "GÃ¶rev: {task}")
        ])
        
        try:
            response = self.llm.invoke(planning_prompt.format_messages(task=state["input"]))
            plan_text = response.content
            
            # Plan metnini parse et
            plan_steps = []
            for line in plan_text.split('\n'):
                line = line.strip()
                if line and any(line.startswith(f"{i}.") for i in range(1, 20)):
                    plan_steps.append(line)
            
            print(f"ğŸ“‹ Plan oluÅŸturuldu: {len(plan_steps)} adÄ±m")
            for i, step in enumerate(plan_steps, 1):
                print(f"   {i}. {step}")
            
            return {
                "plan": plan_steps,
                "current_step_index": 0,
                "executed_steps": []
            }
            
        except Exception as e:
            print(f"âŒ Planlama hatasÄ±: {e}")
            return {
                "plan": ["[HATA] Plan oluÅŸturulamadÄ±"],
                "current_step_index": 0,
                "executed_steps": [],
                "final_result": f"Planlama hatasÄ±: {str(e)}"
            }

    # === Ä°Å Ä°STASYONU 2: Ä°CRA DÃœÄÃœMÃœ ===
    def execute_step(self, state: AgentState) -> Dict:
        """
        Plandaki sÄ±radaki adÄ±mÄ± analiz eder ve ilgili aracÄ± Ã§alÄ±ÅŸtÄ±rÄ±r.
        """
        current_index = state["current_step_index"]
        plan = state["plan"]
        
        if current_index >= len(plan):
            print("âœ… [Ä°CRA DÃœÄÃœMÃœ] TÃ¼m adÄ±mlar tamamlandÄ±!")
            return {"current_step_index": current_index}
        
        current_step = plan[current_index]
        print(f"\nâš¡ [Ä°CRA DÃœÄÃœMÃœ] AdÄ±m {current_index + 1}/{len(plan)}: {current_step}")
        
        try:
            # AdÄ±mdan araÃ§ adÄ±nÄ± ve parametreleri Ã§Ä±kar
            tool_name, tool_params = self._parse_step(current_step, state)
            
            if tool_name in self.tools_dict:
                print(f"ğŸ”§ AraÃ§ '{tool_name}' Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
                
                # AracÄ± Ã§alÄ±ÅŸtÄ±r
                if tool_name in ["find_and_prepare_gpu"]:
                    result = self.tools_dict[tool_name].invoke(tool_params)
                elif tool_name in ["execute_command_on_pod"]:
                    result = self.tools_dict[tool_name].invoke(tool_params)
                else:
                    result = self.tools_dict[tool_name](**tool_params)
                
                step_result = {
                    "step_number": current_index + 1,
                    "step_description": current_step,
                    "tool_used": tool_name,
                    "result": result,
                    "status": "success"
                }
                print(f"âœ… AdÄ±m {current_index + 1} baÅŸarÄ±yla tamamlandÄ±")
                
            else:
                step_result = {
                    "step_number": current_index + 1,
                    "step_description": current_step,
                    "tool_used": tool_name,
                    "result": f"Bilinmeyen araÃ§: {tool_name}",
                    "status": "error"
                }
                print(f"âŒ Bilinmeyen araÃ§: {tool_name}")
            
            return {
                "executed_steps": [step_result],
                "current_step_index": current_index + 1
            }
            
        except Exception as e:
            print(f"âŒ AdÄ±m {current_index + 1} hatasÄ±: {e}")
            error_result = {
                "step_number": current_index + 1,
                "step_description": current_step,
                "result": f"Hata: {str(e)}",
                "status": "error"
            }
            
            return {
                "executed_steps": [error_result],
                "current_step_index": current_index + 1
            }

    def _parse_step(self, step: str, state: AgentState) -> tuple:
        """
        AdÄ±m metninden araÃ§ adÄ±nÄ± ve parametreleri Ã§Ä±karÄ±r.
        """
        # [ARAÃ‡_ADI] formatÄ±nÄ± ara
        if '[' in step and ']' in step:
            start = step.index('[') + 1
            end = step.index(']')
            tool_name = step[start:end]
            description = step[end+1:].strip()
        else:
            # Fallback
            tool_name = "decide_architecture"
            description = step
        
        # Parametreleri akÄ±llÄ±ca belirle
        tool_params = {}
        
        if tool_name == "find_and_prepare_gpu":
            # VRAM miktarÄ±nÄ± metinden Ã§Ä±kar
            if "16GB" in description or "16 GB" in description:
                tool_params = {"min_memory_gb": 16}
            elif "32GB" in description or "32 GB" in description:
                tool_params = {"min_memory_gb": 32}
            else:
                tool_params = {"min_memory_gb": 16}  # varsayÄ±lan
                
        elif tool_name == "execute_command_on_pod":
            # Ã–nceki adÄ±mlardan Pod ID'sini bul
            pod_id = self._extract_pod_id_from_history(state["executed_steps"])
            
            if "git clone" in description.lower():
                command = "git clone https://github.com/pytorch/pytorch.git"
            elif "python" in description.lower() and "main" in description.lower():
                command = "cd pytorch && python setup.py install"
            else:
                command = "echo 'Command execution simulation'"
            
            tool_params = f"{pod_id},{command}" if pod_id else "unknown_pod,echo 'No pod found'"
            
        return tool_name, tool_params

    def _extract_pod_id_from_history(self, executed_steps: List[Dict]) -> str:
        """
        Ã–nceki adÄ±mlardan Pod ID'sini bulur.
        """
        for step in executed_steps:
            if step.get("tool_used") == "find_and_prepare_gpu":
                result = step.get("result", {})
                if isinstance(result, dict) and "pod_info" in result:
                    return result["pod_info"].get("id", "")
        return ""

    # === Ä°Å Ä°STASYONU 3: RAPORLAMA DÃœÄÃœMÃœ ===
    def generate_response(self, state: AgentState) -> Dict:
        """
        TÃ¼m adÄ±mlarÄ±n sonuÃ§larÄ±nÄ± Ã¶zetleyerek nihai cevap oluÅŸturur.
        """
        print("\nğŸ“Š [RAPORLAMA DÃœÄÃœMÃœ] Nihai rapor hazÄ±rlanÄ±yor...")
        
        executed_steps = state["executed_steps"]
        original_task = state["input"]
        
        # Ã–zet oluÅŸtur
        summary_parts = [f"**GÃ¶rev:** {original_task}\n"]
        
        success_count = 0
        error_count = 0
        
        for step in executed_steps:
            step_num = step.get("step_number", "?")
            status = step.get("status", "unknown")
            description = step.get("step_description", "")
            
            if status == "success":
                success_count += 1
                summary_parts.append(f"âœ… **AdÄ±m {step_num}:** {description}")
            else:
                error_count += 1
                summary_parts.append(f"âŒ **AdÄ±m {step_num}:** {description}")
        
        # Genel durum
        if error_count == 0:
            final_status = f"ğŸ‰ **BAÅARILI!** TÃ¼m {success_count} adÄ±m baÅŸarÄ±yla tamamlandÄ±."
        else:
            final_status = f"âš ï¸ **KISMÄ° BAÅARILI:** {success_count} adÄ±m baÅŸarÄ±lÄ±, {error_count} adÄ±m hatalÄ±."
        
        summary_parts.insert(1, final_status + "\n")
        
        final_result = "\n".join(summary_parts)
        
        print("ğŸ“‹ Nihai rapor oluÅŸturuldu!")
        return {"final_result": final_result}

    # === KARAR VERÄ°CÄ° ===
    def should_continue_execution(self, state: AgentState) -> str:
        """
        Planda daha adÄ±m var mÄ± kontrol eder.
        """
        current_index = state["current_step_index"]
        total_steps = len(state["plan"])
        
        if current_index < total_steps:
            print(f"ğŸ”„ [KARAR] Devam: {current_index}/{total_steps} adÄ±m tamamlandÄ±")
            return "continue"
        else:
            print(f"ğŸ [KARAR] BitiÅŸ: TÃ¼m {total_steps} adÄ±m tamamlandÄ±")
            return "generate_response"

    # === YENÄ° ÅEHIR HARÄ°TASI (AkÄ±llÄ± YÃ¶nlendirmeli Graf OluÅŸturucu) ===
    def build_graph(self):
        """
        AkÄ±llÄ± yÃ¶nlendirme sistemi ile Ã§ok adÄ±mlÄ± iÅŸ akÄ±ÅŸÄ±nÄ±n grafiÄŸini oluÅŸturur.
        """
        print("ğŸ—ºï¸ GraphAgent haritasÄ± Ã§iziliyor...")
        
        workflow = StateGraph(AgentState)
        
        # YENÄ° Ä°Å Ä°STASYONLARI: AkÄ±llÄ± yÃ¶nlendirme sistemi
        workflow.add_node("route_query", self.route_query)      # GÃ¼venlik gÃ¶revlisi
        workflow.add_node("chatbot_step", self.chatbot_step)    # Sohbet masasÄ±
        
        # ESKÄ° Ä°Å Ä°STASYONLARI: KarmaÅŸÄ±k gÃ¶rev iÅŸleme sistemi  
        workflow.add_node("plan_step", self.plan_step)
        workflow.add_node("execute_step", self.execute_step) 
        workflow.add_node("generate_response", self.generate_response)
        
        # YENÄ° BAÅLANGIÃ‡ NOKTASI: ArtÄ±k gÃ¼venlik gÃ¶revlisi kapÄ±da!
        workflow.set_entry_point("route_query")
        
        # YENÄ° AKILLI YOLLAR: KoÅŸullu yÃ¶nlendirme sistemi
        workflow.add_conditional_edges(
            "route_query",
            lambda state: state["route_decision"],
            {
                "chat": "chatbot_step",        # Basit sohbet â†’ Sohbet masasÄ±
                "task": "plan_step"           # KarmaÅŸÄ±k gÃ¶rev â†’ Planlama bÃ¶lÃ¼mÃ¼  
            }
        )
        
        # SOHBET YOLU: Direkt bitiÅŸe gidiyor (hiÃ§ araÃ§ kullanmÄ±yor)
        workflow.add_edge("chatbot_step", END)
        
        # GÃ–REV YOLU: Eskiden olduÄŸu gibi karmaÅŸÄ±k sÃ¼reÃ§
        workflow.add_edge("plan_step", "execute_step")
        
        workflow.add_conditional_edges(
            "execute_step",
            self.should_continue_execution,
            {
                "continue": "execute_step",              # DÃ¶ngÃ¼: Bir sonraki adÄ±ma
                "generate_response": "generate_response" # Bitirme: Raporlama
            }
        )
        
        workflow.add_edge("generate_response", END)
        
        print("âœ… Graf baÅŸarÄ±yla oluÅŸturuldu!")
        return workflow.compile()

    def run(self, query: str) -> Dict:
        """
        AkÄ±llÄ± yÃ¶nlendirmeli gÃ¶rev yÃ¼rÃ¼tÃ¼cÃ¼sÃ¼.
        """
        print(f"\nğŸš€ [GÃ–REV BAÅLADI] {query}")
        
        initial_state = {
            "input": query,
            "route_decision": "",     # YENÄ°: YÃ¶nlendirme kararÄ±
            "plan": [],
            "executed_steps": [], 
            "current_step_index": 0,
            "final_result": ""
        }
        
        final_state = self.graph.invoke(initial_state, {"recursion_limit": 50})
        
        print("\nğŸ¯ [GÃ–REV TAMAMLANDI]")
        return {
            "result": final_state.get("final_result", "SonuÃ§ oluÅŸturulamadÄ±"),
            "intermediate_steps": final_state.get("executed_steps", []),
            "plan": final_state.get("plan", [])
        }


# --- Test BloÄŸu ---
if __name__ == '__main__':
    print("ğŸ§ª === Ã‡OK ADIMLI GRAPH AGENT TESLÄ°MÄ° ===")
    
    graph_agent = GraphAgent()
    
    # KarmaÅŸÄ±k test gÃ¶revi
    test_query = (
        "Bana 16GB VRAM'li bir GPU ortamÄ± hazÄ±rla, "
        "ardÄ±ndan PyTorch repository'sini clone et ve kurulumunu yap."
    )
    
    print(f"\nğŸ“ Test GÃ¶revi: {test_query}")
    result_state = graph_agent.run(test_query)
    
    print("\n" + "="*80)
    print("ğŸ¯ SONUÃ‡:")
    print(result_state.get("result", "SonuÃ§ bulunamadÄ±"))
    print("="*80)
    
    # Plan ve adÄ±mlarÄ± da gÃ¶ster
    if "plan" in result_state:
        print(f"\nğŸ“‹ OluÅŸturulan Plan ({len(result_state['plan'])} adÄ±m):")
        for i, step in enumerate(result_state["plan"], 1):
            print(f"  {i}. {step}")
    
    if "intermediate_steps" in result_state:
        print(f"\nâš¡ GerÃ§ekleÅŸtirilen AdÄ±mlar ({len(result_state['intermediate_steps'])}):")
        for step in result_state["intermediate_steps"]:
            status = "âœ…" if step.get("status") == "success" else "âŒ"
            print(f"  {status} AdÄ±m {step.get('step_number', '?')}: {step.get('step_description', 'N/A')}")

